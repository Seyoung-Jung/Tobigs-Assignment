{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"},"colab":{"name":"HW2_week8_CNN_basic.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"tNmHGi4rVJqp","colab_type":"text"},"source":["### Assignment 2\n","# Alexnet Model 구현"]},{"cell_type":"code","metadata":{"id":"8IMZq-IGVJqr","colab_type":"code","colab":{}},"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"g1Io4olAVJqy","colab_type":"code","colab":{}},"source":["class Net(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.architecture = nn.Sequential(\n","            nn.Conv2d(3, 96, (11,11), stride=(4,4)),\n","            nn.ReLU(True),\n","            nn.MaxPool2d(kernel_size=(3,3), stride=(2,2)), \n","            \n","            nn.Conv2d(96, 256, (5,5), stride=(1,1), padding=2),\n","            nn.ReLU(True),\n","            nn.MaxPool2d(kernel_size=(3,3), stride=(2,2)), \n","            \n","            nn.Conv2d(256, 384, (3,3), stride=(1,1), padding=1),\n","            nn.ReLU(True),\n","            nn.Conv2d(384, 384, (3,3), stride=(1,1), padding=1),\n","            nn.ReLU(True),\n","            nn.Conv2d(384, 256, (3,3), stride=(1,1), padding=1),\n","            nn.ReLU(True),\n","            nn.MaxPool2d(kernel_size=(3,3), stride=(2,2)), \n","            \n","            nn.Flatten(),\n","            \n","            nn.Linear(9216, 4096),\n","            nn.ReLU(True),\n","            \n","            nn.Linear(4096, 4096),\n","            nn.ReLU(True),\n","            \n","            nn.Linear(4096, 1000),\n","        )\n","        \n","    def forward(self, x):\n","        x = self.architecture(x)\n","        return F.log_softmax(x)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aeWviHncVJq2","colab_type":"code","colab":{}},"source":["model = Net()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OZuzR8sjVJq7","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":618},"executionInfo":{"status":"ok","timestamp":1600008045581,"user_tz":-540,"elapsed":643,"user":{"displayName":"정세영","photoUrl":"","userId":"10410157789754532104"}},"outputId":"9c894416-27d5-4f19-da38-8a6669059da1"},"source":["from torchsummary import summary\n","\n","summary(model,(3,227,227))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["----------------------------------------------------------------\n","        Layer (type)               Output Shape         Param #\n","================================================================\n","            Conv2d-1           [-1, 96, 55, 55]          34,944\n","              ReLU-2           [-1, 96, 55, 55]               0\n","         MaxPool2d-3           [-1, 96, 27, 27]               0\n","            Conv2d-4          [-1, 256, 27, 27]         614,656\n","              ReLU-5          [-1, 256, 27, 27]               0\n","         MaxPool2d-6          [-1, 256, 13, 13]               0\n","            Conv2d-7          [-1, 384, 13, 13]         885,120\n","              ReLU-8          [-1, 384, 13, 13]               0\n","            Conv2d-9          [-1, 384, 13, 13]       1,327,488\n","             ReLU-10          [-1, 384, 13, 13]               0\n","           Conv2d-11          [-1, 256, 13, 13]         884,992\n","             ReLU-12          [-1, 256, 13, 13]               0\n","        MaxPool2d-13            [-1, 256, 6, 6]               0\n","          Flatten-14                 [-1, 9216]               0\n","           Linear-15                 [-1, 4096]      37,752,832\n","             ReLU-16                 [-1, 4096]               0\n","           Linear-17                 [-1, 4096]      16,781,312\n","             ReLU-18                 [-1, 4096]               0\n","           Linear-19                 [-1, 1000]       4,097,000\n","================================================================\n","Total params: 62,378,344\n","Trainable params: 62,378,344\n","Non-trainable params: 0\n","----------------------------------------------------------------\n","Input size (MB): 0.59\n","Forward/backward pass size (MB): 11.06\n","Params size (MB): 237.95\n","Estimated Total Size (MB): 249.60\n","----------------------------------------------------------------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:34: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"jmLEkDM0742O","colab_type":"text"},"source":["전반적인 layer 구조를 살펴보면, Convolution layer와 maxpooling layer가 겹겹이 쌓여있고 flatten 이후에 linear (Full Connected Layer)로 구성되어있다.\n","\n","Maxpooling layer와 마지막 softmax를 제외한 모든 layer에서는 ReLU activation 함수를 통해 output값을 내준다. \n","\n","output shape을 살펴보면, 넓었던 input shape에서 점차 좁고 길어진다는 것을 알 수 있다. shape이 길어질수록 학습해야하는 parameter도 많아지는 것을 발견할 수 있다.\n","\n","parameter를 살펴보면 Convolution layer와 Linear layer에만 존재하는 것을 알 수 있는데, 이는 Maxpooling과 ReLU에서는 업데이트할 가중치가 존재하지 않기 때문이다.\n","\n","또한 Flat한 Linear layer의 parameter가 Convolution layer에 비해 압도적으로 많은 것을 알 수 있는데, 이것이 CNN이 적은 parameter로도 공간정보를 잘 활용한다는 반증이다."]}]}